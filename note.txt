查看端口占用情况并杀掉
netstat junlp | grep 端口号
kill -9 进程号

M2E - http://download.eclipse.org/technology/m2e/releases

红杏邀请链接：honx.in/_VX_To4kWGkiiyw28

scp -r share@192.168.225.109:.m2/repository .m2/

运行lombok安装到eclipse中： java -jar ~/Downloads/tar/lombok.jar

openJDK
/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/security
/usr/lib/jvm/java-7-oracle/jre/lib/security
/usr/lib/jvm/java-7-openjdk-amd64/jre/lib/security
/etc/security
/etc/java-6-openjdk/security
/etc/java-7-openjdk/security


select http_code,count(http_code) as count from (select get_json_object(sls_extract_others, '$.http_code') as http_code,sls_time from wx_accesslogs) temp group by http_code;

ODPS SQL 封装了解析JSON的内建函数
select get_json_object(sls_extract_others, "$.http_code") from wx_accesslogs;

在ODPS中创建jar资源
create resource jar /home/user/Documents/http_status_count.jar
create resource jar /home/user/Documents/my_date_format.jar;

执行ODPS中的jar资源
jar -resources http_status_count.jar -classpath /home/user/Documents/http_status_count.jar com.augmentum.odps.sql.HttpStatusCount 2015-05-30 2015-06-01

注册UFT
create function decrypt com.augmentum.odps.udf.DecryptUDF decrypt.jar

执行UFT函数
select format_date(operated_date), http_code, count from wx_accesslogs_http_code_result;

使用正则表达式 对内容进行抽取.
select regexp_extract(sls_extract_others,'<Encrypt>(.*?)</Encrypt>',1) from tmp;
创建中间表1
create table if not exists tmp (sls_source string,sls_time bigint,sls_topic string,proxy_ip string,time string,http_method string,http_path string,http_code bigint,rsp_size bigint,rsp_time string,proxy_rsp_time string,client_ip string,body string) partitioned by (sls_partition_time string);
创建中间表2
create table if not exists tmp (sls_source string,sls_time bigint,sls_topic string,proxy_ip string,time string,http_method string,http_path string,http_code bigint,rsp_size bigint,rsp_time string,proxy_rsp_time string,client_ip string,body_encrypt string) partitioned by (sls_partition_time string);

向中间表1中插入数据(动态插入分区数据)
insert overwrite table tmp partition(sls_partition_time) select sls_source,sls_time,sls_topic,get_json_object(sls_extract_others,'$.proxy_ip'),get_json_object(sls_extract_others,'$.time'),get_json_object(sls_extract_others,'$.http_method'),get_json_object(sls_extract_others,'$.http_path'),get_json_object(sls_extract_others,'$.http_code'),get_json_object(sls_extract_others,'$.rsp_size'),get_json_object(sls_extract_others,'$.rsp_time'),get_json_object(sls_extract_others,'$.proxy_rsp_time'),get_json_object(sls_extract_others,'$.client_ip'),get_json_object(sls_extract_others,'$.body'),sls_partition_time from wx_accesslogs;
中间表2
insert overwrite table tmp partition(sls_partition_time) select sls_source,sls_time,sls_topic,proxy_ip,time,http_method,http_path,http_code,rsp_size,rsp_time,proxy_rsp_time,client_ip,xml_content_format(regexp_extract(body,'<Encrypt>(.*?)</Encrypt>',1)),sls_partition_time from tmp;

获取中间表中Body中的Encrypt内容
select string_format(regexp_extract(body,'<Encrypt>(.*?)</Encrypt>',1)) from tmp;
select string_format(encrypted_text) from (select regexp_extract(body,'<Encrypt>(.*?)</Encrypt>',1) as encrypted_text from tmp) abc;

对字符串进行分割,从第10位开始,截取长度为15
select substr(to_user_name,10,15) from tmp limit 5;

Unixtime转化为datatime
select from_unixtime(operated_date) from tmp;

向中间表tmp中第一次插入数据(得到Body内容)
insert overwrite table tmp partition(sls_partition_time) select sls_source,sls_time,sls_topic,get_json_object(sls_extract_others,'$.proxy_ip'),get_json_object(sls_extract_others,'$.time'),get_json_object(sls_extract_others,'$.http_method'),get_json_object(sls_extract_others,'$.http_path'),get_json_object(sls_extract_others,'$.http_code'),get_json_object(sls_extract_others,'$.rsp_size'),get_json_object(sls_extract_others,'$.rsp_time'),get_json_object(sls_extract_others,'$.proxy_rsp_time'),get_json_object(sls_extract_others,'$.client_ip'),get_json_object(sls_extract_others,'$.body'),sls_partition_time from wx_accesslogs where sls_time >= 1433001600 and sls_time < 1433260800;
向中间表tmp中第二次插入数据(得到形如<![DTD...]]的Encrypt内容)
insert overwrite table tmp partition(sls_partition_time) select sls_source,sls_time,sls_topic,proxy_ip,time,http_method,http_path,http_code,rsp_size,rsp_time,proxy_rsp_time,client_ip,regexp_extract(body,'<Encrypt>(.*?)</Encrypt>',1),sls_partition_time from tmp;
向中间表tmp中第三次插入数据(得到Encrypt的内容)
insert overwrite table tmp partition(sls_partition_time) select sls_source,sls_time,sls_topic,proxy_ip,time,http_method,http_path,http_code,rsp_size,rsp_time,proxy_rsp_time,client_ip,xml_content_format(body),sls_partition_time from tmp;

insert overwrite table tmp partition(sls_partition_time) select sls_source,sls_time,sls_topic,proxy_ip,time,http_method,http_path,http_code,rsp_size,rsp_time,proxy_rsp_time,client_ip,body,sls_partition_time from tmp where sls_time >= 1433001600 and sls_time < 1433260800;


经过上面一条SQL语句的操作后仍然存在一个\,继续替换
insert overwrite table tmp select sls_time, regexp_replace(body,'\\','',0) from tmp;



表wx_accesslogs_detail DLL语句
create table if not exists wx_accesslogs_detail(sls_source string,sls_time bigint,sls_topic string,proxy_ip string,time string,http_method string,http_path string,http_code bigint,rsp_size bigint,rsp_time double,proxy_rsp_time double,client_ip string,to_user_name string,from_user_name string,create_time bigint,msg_type string,event string,latitude double,longitude double,precision double,event_key string,ticket string,content string,media_id bigint,format_ string,thumb_media_id bigint,location_x double,location_y double,scale bigint,label string,title string,description string,url string,msg_id bigint,status string,total_count bigint,filter_count bigint,sent_count bigint,error_count bigint) partitioned by (sls_partition_time string);

insert overwrite table wx_accesslogs_detail partition(sls_partition_time)select sls_source,sls_time,sls_topic,proxy_ip,time,http_method,http_path,http_code,rsp_size,rsp_time,proxy_rsp_time,client_ip,'gh_3a3787c421e9','o2c06t5a0LKwJiBVpvjIQjxJ1ViI',1433578956','event','LOCATION',36.671459,116.983833,90.000000','null','null','null',null,'null',null,null,null,null,'null','null','null','null',null,'null',null,null,null,null,sls_partition_time from tmp limit 1;

ODPS中用于处理分页SQL
select body from (select body,row_number() over (partition by sls_topic order by sls_time desc,sls_source) as rank from tmp) sub where rank < 20;


{"proxy_ip":"100.97.176.148","time":"05/Jun/2015:10:37:12 +0800","http_method":"POST","http_path":"/wechat/wx9939f26f206cd491?signature=c8dc107844d81706156e66be5b7c9136926baec9&timestamp=1433471832&nonce=1771387748&encrypt_type=aes&msg_signature=e8a63bc82d40a7fe5f95d9b2fb8c70b13f757f23","http_code":"200","rsp_size":"286","rsp_time":"0.008","proxy_rsp_time":"0.008","client_ip":"140.207.54.80","body":"<xml>\\x0A    <ToUserName><![CDATA[gh_3a3787c421e9]]></ToUserName>\\x0A    <Encrypt><![CDATA[bhEH7Fp0ugzIXkpd9kK5SFAmVgVW8nPL0r2DIEzmvj9frQZTHML8WGnpmAa7VxwlIzg5YWlx/VXUhBMPWA0AGCpAmGnEzCscT7a2wRQrptBJCq3sP/N9OResAeMDNe7MJEh0d6qnFiuJ6w/84fmF8SYO//S4PUNuc8spN3nt3xjjTb7pGQfcHavCYgcRpifaWkgCQvmk8DeC6Y2qudkhiXYJcdlfeydsKmCcRTwm3FqCnI83hSKRzEYnxOtnP1JHIbaLdL+RXMGp672IbbZ03TbNEzbsXKARANJlqyptzbcvMZVhFvxDyvE3aEKh5waazgXmWO9fG/8HvylKDArioIdWBmxS2S7lszSvkvvzmGqs0HvPX0tzRSKsW3gI25x7R9r39lvQonETNdU1GFfuYZQAzqr4a2AgwkDy10x2iJ+pQQxTHOxv8s4yEEVVQq5/3pswM+Igk0nnsjg+1c0wyMHoWsibIzb+1fZrNnbF8vPRu1b/19TQfxUjf1lzRTOE]]></Encrypt>\\x0A</xml>\\x0A"}

select sls_extract_others,sls_partition_time from wx_accesslogs where sls_partition_time = '2015_06_02_15_00';

Eclipse 应用位置 /usr/lib/eclipse/plugins/
ODPS
    Access Key ID:swsWVDPsVdyASRSo
    Access Key Secret:lupLCzAmWKmQ4q4JRxafKkNrEsPI2C
    Project Name:byron_project

    http://service-corp.odps.aliyun-inc.com/api
    yourAccessId
    yourAccessKey

Set<Entry<String, Object>> resultSet = map.entrySet();
Iterator<Entry<String, Object>> iterator = resultSet.iterator();
List<Map<String, String[]>> resultMap = new ArrayList<Map<String, String[]>>();

getconf LONG_BIT 查看操作系统位数
lsb_release -a 查看系统当前版本号

开放数据处理服务（ODPS）
   ODPS的目的是为用户提供一种便捷的分析处理海量数据的手段。用户可以不必关心分布式计算细节，从而达到分析大数据的目的。 ODPS已经在阿里巴巴集团内部得到大规模应用，例如：大型互联网企业的数据仓库和BI分析、网站的日志分析、电子商务网站的交易分析、用户特征和兴趣挖掘等
   用处：可以提供海量数据仓库的解决方案以及针对大数据的分析建模服务。

ODPS只能以表的形式存储数据，并对外提供了SQL查询功能。

Hadoop
   Hadoop是一个由Apache基金会所开发的分布式系统基础架构。
用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力进行高速运算和存储。
Hadoop的框架最核心的设计就是：HDFS和MapReduce。HDFS为海量的数据提供了存储，则MapReduce为海量的数据提供了计算。

MapReduce
   MapReduce是处理大量半结构化数据集合的编程模型。编程模型是一种处理并结构化特定问题的方式。例如，在一个关系数据库中，使用一种集合语言执行查询，如SQL。告诉语言想要的结果，并将它提交给系统来计算出如何产生计算。还可以用更传统的语言(C++，Java)，一步步地来解决问题。这是两种不同的编程模型，MapReduce就是另外一种。

ODPS的数据上传/下载方式
1） dship工具，dship是基于本地文件的数据上传下载工具，可以大文件上传或下载至ODPS。
2） ODPS Tunnel，灵活性高，适用于任意数据源和复杂的场景，需要开发者写程序实现。

上传数据的大体步骤:大体来看，可以分为以下5步：
a) 准备源数据，例如源文件或数据表；
b) 设计表结构和分区定义，作数据类型转换，然后在ODPS上创建表；
c) 在ODPS表上添加分区，如果没有分区，则忽略此步骤。假如使用日期作分区，则添加分区如“20140312”。
d) 把数据上传到指定分区或表上；
e) 本次上传结束。

ODPS SQL
   如果用户想创建表，需要通过SQLTask接口，而不是 Table 接口。用户需要将 创建表(CREATE TABLE) 的语句传入SQLTask。

[limit number]的number是常数，限制输出行数。当使用无limit的select语句直接从屏幕输出查看结果时，最多只输出5000行。 每个项目空间的这个屏显最大限制限制可能不同，可以通过控制台面板控制。

insertSql = "insert into table wx_accesslogs_detail partition(sls_partition_time)" +
								"select sls_source,sls_time,sls_topic,proxy_ip,time,http_method,http_path,http_code,rsp_size,rsp_time,proxy_rsp_time,client_ip," +
								data.get(TOUSERNAME) != null ? "'" + data.get(TOUSERNAME) + "'" : "null," +
								data.get(FROMUSERNAME) != null ? "'" + data.get(FROMUSERNAME) + "'" : "null," +
								data.get(CREATETIME) + "," +
								data.get(MSGTYPE) != null ? "'" + data.get(MSGTYPE) + "'" : "null," +
								data.get(EVENT) != null ? "'" + data.get(EVENT) + "'" : "null," +
								data.get(LATITUDE) + "," +
								data.get(LONGITUDE) + "," +
								data.get(PRECISION) + "," +
								data.get(EVENTKEY) != null ? "'" + data.get(EVENTKEY) + "'" : "null," +
							    data.get(TICKET) != null ? "'" + data.get(TICKET) + "'" : "null," +
								data.get(CONTENT) != null ? "'" + data.get(CONTENT) + "'" : "null," +
								data.get(MEDIAID) + "," +
								data.get(FORMAT) != null ? "'" + data.get(FORMAT) + "'" : "null," +
								data.get(THUMBMEDIAID) + "," +
								data.get(LOCATIONX) + "," +
								data.get(LOCATIONY) + "," +
								data.get(SCALE) + "," +
								data.get(LABEL) != null ? "'" + data.get(LABEL) + "'" : "null," +
								data.get(TITLE) != null ? "'" + data.get(TITLE) + "'" : "null," +
								data.get(DESCRIPTION) != null ? "'" + data.get(DESCRIPTION) + "'" : "null," +
								data.get(URL) != null ? "'" + data.get(URL) + "'" : "null," +
								data.get(MSGID) + "," +
								data.get(STATUS) != null ? "'" + data.get(STATUS) + "'" : "null," +
								data.get(TOTALCOUNT) + "," +
								data.get(FILTERCOUNT) + "," +
								data.get(SENTCOUNT) + "," +
								data.get(ERRORCOUNT) + ",sls_partition_time from " +
								"(select *,row_number() over (partition by sls_topic order by sls_time desc,sls_source) as rank from tmp where sls_time >= " +
								CommonUtils.convertToUnixTime(beginDate) + " and sls_time < " +
								CommonUtils.convertToUnixTime(CommonUtils.addDays(endDate, 1)) + ") sub " +
								"where rank = " + (beginIndex + i) + ";";

ERROR: ODPS-0010000:System internal error - build/release64/pangu/client/file_input_stream_impl.cpp(705): PanguStreamCorruptedException: There is no usable replica! file name pangu://AY52:10240/product/odps/we_connect/data/tmp/sls_partition_time=2015_05_30_11_00/1433836864.206, chunk index 0, file id 358975054521827413
Stack trace:
/apsara/lib64/libpangu_shared.so.42(apsara::pangu::FileInputStreamImpl::PrepareReadRequest(apsara::pangu::DataFetcher&)+0x70b) [0x7f7e94704aeb]
/apsara/lib64/libpangu_shared.so.42(apsara::pangu::FileInputStreamImpl::requestByWorkItem(std::tr1::shared_ptr<apsara::pangu::DataFetcher>&)+0x19d) [0x7f7e9471521d]
/apsara/lib64/libpangu_shared.so.42(apsara::pangu::FileInputStreamImpl::doAsyncRead(std::tr1::shared_ptr<apsara::pangu::ClientApiContext>&, long, void*, int, apsara::common::Closure<void, apsara::pangu::AsyncStatus, int, void, void, void, void, void, void>*)+0x646) [0x7f7e947168b6]
/apsara/lib64/libpangu_shared.so.42(apsara::pangu::FileInputStreamImpl::ReadInternal(std::tr1::shared_ptr<apsara::pangu::ClientApiContext>&, void*, int)+0x4f8) [0x7f7e94717668]
/apsara/lib64/libpangu_shared.so.42(apsara::pangu::FileInputStreamImpl::Read(void*, int)+0x2cf) [0x7f7e947190df]
lib/libodps_cfile.so(apsara::odps::storage::HeadLoader::GetFileType(std::tr1::shared_ptr<apsara::pangu::FileInputStream> const&)+0x4b) [0x7f7e92d055bb]
lib/libodps_cfile.so(apsara::odps::storage::ISQLRecordReaderWriterFactory::CreateSQLRecordReaderByOffset(std::tr1::shared_ptr<apsara::pangu::FileInputStream> const&, apsara::odps::SQLRecordSchema const*, unsigned long, unsigned long, unsigned long, std::string const&)+0x39) [0x7f7e92d5f069]
lib/libodps_cfile.so(apsara::odps::storage::ISQLRecordReaderWriterFactory::CreateSQLRecordReaderByOffset(std::string const&, apsara::odps::SQLRecordSchema const*, unsigned long, unsigned long, unsigned long, std::string const&)+0x6e) [0x7f7e92d5fc4e]
lib/libcganjiang.so(cganjiang::mapreduce::CfileDeserializerWithSQLRecord::Deserialize(apsara::Serializable&)+0x376) [0x7f7e9b07ff16]
lib/libcganjiang.so(cganjiang::mapreduce::BriefFileReader::Read()+0x1a) [0x7f7e9afbe8fa]
/apsara/lib64/libfuxi_shared.so.44(apsara::fuxi::RecordSequentialReader::Read()+0x10e) [0x7f7e99cdc81e]
lib/libcganjiang.so(cganjiang::mapreduce::CGanjiangReader::Read()+0x86) [0x7f7e9aff3286]
lib/libcganjiang.so(cganjiang::mapreduce::CGanjiangReaderWrapper::Read()+0x23) [0x7f7e9b00cd43]
lib/libcganjiang.so(cganjiang::mapreduce::GJMapOperator::Run()+0x59) [0x7f7e9afec9d9]
lib/libcganjiang.so(cganjiang::mapreduce::PlainOperatorRunner::Run()+0x7b) [0x7f7e9b0d56db]
lib/libcganjiang.so(cganjiang::mapreduce::MapTask::Run()+0x8c) [0x7f7e9b02f0dc]
/apsara/tubo/TempRoot/Odps/we_connect_20150610014352107gajomr06_SQL_0_0_0_job0/M1_Stg1@r13c12075.dg.aliyun.com#248/ctasker(apsara::fuxi::WrapperTask::Run()+0xc2f) [0x415a0f]
/apsara/lib64/libfuxi_shared.so.44(apsara::fuxi::TaskWorkerStub::RunInstanceThread(std::string const&, std::tr1::shared_ptr<apsara::fuxi::InstanceDescription>)+0x4897) [0x7f7e99e242b7]
/apsara/lib64/libfuxi_shared.so.44(std::tr1::_Function_handler<apsara::Any ()(), std::tr1::_Bind<std::tr1::_Mem_fn<int (apsara::fuxi::TaskWorkerStub::*)(std::string const&, std::tr1::shared_ptr<apsara::fuxi::InstanceDescription>)> ()(apsara::fuxi::TaskWorkerStub*, std::string, std::tr1::shared_ptr<apsara::fuxi::InstanceDescription>)> >::_M_invoke(std::tr1::_Any_data const&)+0x9a) [0x7f7e99e3cd4a]
/apsara/lib64/libapsara_common_shared.so.40(std::tr1::function<apsara::Any ()()>::operator()() const+0x11) [0x7f7e995d19a1]
/apsara/lib64/libapsara_common_shared.so.40(apsara::ThreadWrapperFunction(void*)+0x5d) [0x7f7e995d170d]
/lib64/libpthread.so.0 [0x304bc0677d]
/lib64/libc.so.6(clone+0x6d) [0x304b4d49ad]


不加commons-codec-1.9.jar后
java.lang.NoClassDefFoundError: org/apache/commons/codec/binary/Base64
	at com.augmentum.odps.udf.WXBizMsgCrypt.<init>(DecryptUDF.java:39)
	at com.augmentum.odps.udf.DecryptUDF.evaluate(DecryptUDF.java:27)
Caused by: java.lang.ClassNotFoundException: org.apache.commons.codec.binary.Base64
	at java.net.URLClassLoader$1.run(Unknown Source)
	at java.net.URLClassLoader$1.run(Unknown Source)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(Unknown Source)
	at java.lang.ClassLoader.loadClass(Unknown Source)
	at com.alibaba.apsara.sandking.SandboxLauncher$AppClassLoader.loadClass(SandboxLauncher.java:250)
	at java.lang.ClassLoader.loadClass(Unknown Source)
	... 2 more

加入commons-codec-1.9.jar后
ERROR: ODPS-0123131:User defined function exception - Unable to load java class com.augmentum.odps.udf.DecryptUDF: loader (instance of  com/aliyun/odps/plan/converter/ClassLoaderHelper): attempted  duplicate class definition for name: "org/apache/commons/codec/BinaryDecoder"

微信加密码
l7oTHlrRLkvQ8nwzsmLQrDxJSngWAMLgx05Tihd8PrDJ0g4CG/yftOt4TRNupdjwf89h1ptz5zPHgtc6PKTnkocpC3x8Bu/SasIFUupcd2ZVPYdrRC/so2IRnZZyDatuYjG1YR4qipXC1cTdIgPe3Bvmk3NiSE2H+UEOXIEkqMbHxBjd12XOyBmBHU6etMQDMrSGiofTxfT6GLf9Zpqe7Acd4+xV+eRKFjutHmxL5E1lf4dfwfYWLgLtJMesgKkVllGNxxk1uqV59luPIRbm3FlfQf188pDeQ0J4F7uqzPDsaT764VI0wvG4dY2DGfEZOh8rnv+Ix0ztc3ffyn9sYVauP69+20d2XuU1S/FK+lZaSTS4SNyw85OL9Qk2aQ0ToWNB/T+jSHTf6WllfKfp/LDD4YUmnRGeQ1ZGkYE105fM/8hR1CaM6ml3M6S7VeVsQM4ShbguVgIgh7FVWy67tS7GFZUb/6ygeUgMoPXaPxV+hUTUYO34SiHGrJvisBzR

F+dP3Hha9dhNohywZl2mScIWfyMIxRoGUJJ5YvPRyNX5LIe6RRv+cmWjCA9rK3lu8yT8+sYQpOZjpz3YRRmA59V5DYLGeqVRqH8Qf3MbXKiNQgGLkBl2tQ7Ahrl7KFssDabzBg/4RPanSVLfjiFTVi4KT7FA3T11Xyi32KqXKRMs0L5lPqi3uRdMJB29lVoQzg0DyqPpFWNpKvShKsv5PySU7mNFIn4L8VKYCZpN8t/qL+q0lB0AvkdMNtVttrtnofrmAVgYhQSC9DRkw6ZbIEuV20cxA70xMQBzp2JUBxgoVUdg3npJfR7ZYd/jH35KSl0xPyitrD9/HHhg05lWZ/ONU79SUI0ZHcnkndjg8H8KhdfGOzXwRIjRFfPtnW0MUDAODt71PHURGsIrAd69mkS22ukZ/Imk+kkxzj4STWK5jmBqHDQW6KqZx4ASDryEXSY86+F1UMs76OcmJlaXZ84m+2lwnMNxIHwEu7EvDuS8Ql6DFj4bR0FoIEUX6Z5F

